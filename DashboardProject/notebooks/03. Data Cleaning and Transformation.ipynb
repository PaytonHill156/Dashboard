{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Transformation\n",
    "\n",
    "In the last chapter, we created functions to download and save the raw data. In this chapter, we take steps to find and clean bad data, and transform it to a structure that is suitable for modeling. We begin by reading in the raw local data with the `run` function.\n",
    "\n",
    "### Important note on importing from solutions\n",
    "\n",
    "Because this is a new chapter, you will need to import functions from the solutions.py file that were defined in the previous chapter. Continue to comment out the import statements that appear after an exercise so that you can practice them. Alternatively, you can put all of your solutions in a solutions2.py file and change the import statements to import your functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from solutions import run\n",
    "data = run()\n",
    "data['world_cases'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['usa_cases'].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the correct columns\n",
    "\n",
    "Take a look at the world and USA DataFrames above and you'll notice a difference in the names and number of columns. The following exercise describes how to select the columns.\n",
    "\n",
    "### Exercise 6\n",
    "\n",
    "<span style=\"color:green; font-size:16px\">Write a function that accepts a single DataFrame and selects the `\"Country/Region\"` column for the world DataFrames, `\"Province_State\"` column for the USA DataFrames, and all the date columns for both. Return a DataFrame with just those columns. Assume that the column names always remain the same for each dataset.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_columns(df):\n",
    "    \"\"\"\n",
    "    Selects the Country/Region column for world DataFrames and\n",
    "    Province_State for USA\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df : DataFrame\n",
    "    \n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this function to select the columns and output from both the world and USA DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solutions import select_columns\n",
    "select_columns(data['world_cases']).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_columns(data['usa_cases']).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the `run` function\n",
    "\n",
    "After each step in this chapter, we'll update our `run` function to pass each DataFrame through the newly created function. Each `run` function will be uniquely labeled with an ending integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7\n",
    "\n",
    "<span style=\"color:green; font-size:16px\">Update the `run` function to include the above step.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run2():\n",
    "    \"\"\"\n",
    "    Run all cleaning and transformation steps\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dictionary of DataFrames\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solutions import run2\n",
    "data = run2()\n",
    "data['usa_cases'].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating area names \n",
    "\n",
    "In both groups of data, there are a few area names that can be updated so that they use a more common name. There are three cruise ships, which we will replace with the string \"Cruise Ship\". Also, since the United States has its own summary table, we can drop it from the world DataFrames.\n",
    "\n",
    "### Exercise 8\n",
    "\n",
    "<span style=\"color:green; font-size:16px\">Write a function that uses the DataFrame `replace` method to replace the names in the first column with the provided dictionary below. Drop all rows from the \"US\" from the world DataFrame.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_AREA = {\n",
    "    \"Korea, South\": \"South Korea\",\n",
    "    \"Taiwan*\": \"Taiwan\",\n",
    "    \"Burma\": \"Myanmar\",\n",
    "    \"Holy See\": \"Vatican City\",\n",
    "    \"Diamond Princess\": \"Cruise Ship\",\n",
    "    \"Grand Princess\": \"Cruise Ship\",\n",
    "    \"MS Zaandam\": \"Cruise Ship\"\n",
    "}\n",
    "\n",
    "def update_areas(df):\n",
    "    \"\"\"\n",
    "    Replace a few of the area names using the REPLACE_AREA dictionary.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df : DataFrame\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9\n",
    "\n",
    "<span style=\"color:green; font-size:16px\">Update the `run` function to include the above step.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run3():\n",
    "    \"\"\"\n",
    "    Run all cleaning and transformation steps\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dictionary of DataFrames\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verify our function by searching for the cruise ships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solutions import run3\n",
    "data = run3()\n",
    "data['usa_cases'].query(\"Province_State == 'Cruise Ship'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate repeating areas\n",
    "\n",
    "In each DataFrame, many areas repeat multiple times as the raw data tracked deaths/cases by the province/state/county level. We desire a single row for each unique area. Complete the exercise below to get the desired result.\n",
    "\n",
    "### Exercise 10\n",
    "\n",
    "<span style=\"color:green; font-size:16px\">Write a function that accepts a single DataFrame, groups by the area column (first column in each DataFrame), and sums up all the date columns.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_area(df):\n",
    "    \"\"\"\n",
    "    Gets a single total for each area\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df : DataFrame\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11\n",
    "\n",
    "<span style=\"color:green; font-size:16px\">Update the `run` function to include the above step.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run4():\n",
    "    \"\"\"\n",
    "    Run all cleaning and transformation steps\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dictionary of DataFrames\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solutions import run4\n",
    "data = run4()\n",
    "data['usa_cases'].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transposing the data to time series\n",
    "\n",
    "We have time series data (a sequence of data over time), but it's not in the customary format where date is along the vertical axis. Complete the following exercise to convert it to a more common format for time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12\n",
    "\n",
    "<span style=\"color:green; font-size:16px\">Write a function that accepts a single DataFrame and transposes it so that the current date columns become the index. Make sure to convert the dates to a datetime data type, since they are strings now.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_to_ts(df):\n",
    "    \"\"\"\n",
    "    Transposes the DataFrame and converts the index to datetime\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df : DataFrame\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13\n",
    "\n",
    "<span style=\"color:green; font-size:16px\">Update the `run` function to include the above step.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run5():\n",
    "    \"\"\"\n",
    "    Run all cleaning and transformation steps\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dictionary of DataFrames\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solutions import run5\n",
    "data = run5()\n",
    "data['usa_cases'].tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding and handling bad data\n",
    "\n",
    "In this section, we will search for bad data and then come up with a solution for handling it. Our DataFrames contain the cumulative count of deaths and cases at each date. These values should never decrease. In order to verify that the values never decrease, we can test whether each day's value is at least as large as all the values preceding it. To do this, we call the `cummax` method which returns the cumulative maximum of each column up to each date. We then compare each value with this cumulative maximum. We'll work with just the world deaths DataFrame for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_deaths = data['world_deaths']\n",
    "bad_data = world_deaths < world_deaths.cummax()\n",
    "bad_data.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If any of these values are `True`, then we've found bad data. Let's sum each column and sort the results to see which columns have the most bad data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_data.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's locate the bad data for Spain, and see if we can find out what's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spain_bad = bad_data['Spain']\n",
    "spain_bad[spain_bad].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect a small subset of the data around the first date of bad data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_deaths.loc['2020-05-21':'2020-05-26', 'Spain']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A drop of nearly 2,000 deaths appears on May 25th. Let's make a plot of Spain's total deaths beginning from the beginning of May to get a better picture of what is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dashboard.mplstyle')\n",
    "world_deaths.loc['2020-05-01':'2020-06-01', 'Spain'].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that almost no new deaths are reported after the sudden decrease on May 25th, until a huge increase in the latter half of June, followed again by a period of very few deaths. Various other data aggregators have reported similar issues with Spain's data.\n",
    "\n",
    "We'll provide a simple solution so that all dates have a value greater than or equal to the prior day's values. In order to have make this replacement, we'll change all the values for dates below the current maximum to missing values with the `mask` method. First, we create a boolean mask, a Series of booleans with the same length as the original Series that meet some criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spain = world_deaths['Spain']\n",
    "mask = spain < spain.cummax()\n",
    "mask.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass this Series to the `mask` method to \"mask\" them - cover them up and replace them with missing values. We show the first 10 dates where the data is now missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spain_masked = spain.mask(mask)\n",
    "spain_masked[spain_masked.isna()].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then linearly interpolate the missing values with the `interpolate` method and plot the updated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spain_masked.interpolate().plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This \"fixes\" the data such that each value is always at least as large as the preceding value. In this particular example, this simple fix doesn't seem to connect the points in a way pleasing to the eye. A better estimation might linearly interpolate from the middle of May to the middle of July. \n",
    "\n",
    "Instead of developing a more complex method to fix bad data, we'll use this simple method and complete a process called **smoothing** later on, which will really help out the model handle these uneven jumps in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing all bad data\n",
    "\n",
    "Let's fix all of the bad data in our DataFrame with the same logic from above, rounding the totals to whole numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = world_deaths < world_deaths.cummax()\n",
    "world_deaths_fixed = world_deaths.mask(mask).interpolate().round(0).astype('int64')\n",
    "world_deaths_fixed.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that all values are at least as large as the previous day's value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = world_deaths_fixed < world_deaths_fixed.cummax()\n",
    "mask.sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 14\n",
    "\n",
    "<span style=\"color:green; font-size:16px\">Write a function that accepts a single DataFrame and fixes all the bad data.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_bad_data(df):\n",
    "    \"\"\"\n",
    "    Replaces all days for each country where the value of\n",
    "    deaths/cases is lower than the current maximum\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 15\n",
    "\n",
    "<span style=\"color:green; font-size:16px\">Update the `run` function to include the above step.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run6():\n",
    "    \"\"\"\n",
    "    Run all cleaning and transformation steps\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dictionary of DataFrames\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that this last step works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solutions import run6\n",
    "data = run6()\n",
    "data['world_cases'].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation complete\n",
    "\n",
    "These steps complete the data preparation process. Let's use one of our previous functions to write this prepared data to the `data/prepared` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solutions import write_data\n",
    "write_data(data, 'data/prepared', index=True, index_label='date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encapsulate all steps into a single class\n",
    "\n",
    "All of the steps in the last two chapters may be encapsulated into a single class.\n",
    "\n",
    "### Exercise 16\n",
    "\n",
    "<span style=\"color:green; font-size:16px\">Write a class that has a method for each of the steps from the last two chapter. Add a `run` method that runs all of the steps and returns the dictionary of DataFrames. Initialize the object with the `download_new` boolean, which allows the user to decide whether to download new data from the online repository or read in the local data. Check the `prepare.py` file for the solution.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareData:\n",
    "    def __init__(self, download_new=True):\n",
    "        self.download_new = download_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check your work run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prepare import PrepareData\n",
    "prepare_data = PrepareData()\n",
    "data = prepare_data.run()\n",
    "data['world_deaths'].head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
