{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Smoothing\n",
    "\n",
    "In this chapter, we'll learn various methods for smoothing the data, which is an important step to take before attempting to build a predictive model.\n",
    "\n",
    "## Smoothing the data\n",
    "\n",
    "The daily reported data has a tremendous amount of variation due to many factors (weekdays vs weekends, holidays, and other reporting delays). Our goal when modeling is to understand the **general trend** of the data and not the variations due to reporting. Let's instantiate our `PrepareData` class and use it to read in the dictionary of all four DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dashboard.mplstyle')\n",
    "\n",
    "from prepare import PrepareData\n",
    "data = PrepareData(download_new=False).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing Texas's data\n",
    "\n",
    "To show how smoothing works, we'll examine the cases in Texas. Let's get an overall view of cumulative cases with a line plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_cases = data['usa_cases']\n",
    "texasc = usa_cases['Texas']\n",
    "texasc.plot(kind='line', title=\"Texas - Cumulative Cases\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except for a few small bumps, the graph looks fairly smooth. Let's investigate further and use the `diff` method to find the daily cases. By default, `diff` returns the difference between the current and previous values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texasc_daily = texasc.diff()\n",
    "texasc_daily.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first value will always be missing when finding the 1-day difference as there is no data from the previous day. Also, there were no cases for the first several weeks of recorded data in Texas. Let's find the last date where no cases were recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_zero_date = texasc[texasc == 0].index[-1]\n",
    "last_zero_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter the data from to show this date onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texasc = texasc.loc[last_zero_date:]\n",
    "texasc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we use the `diff` method, we can simply drop the first missing value to get only the days from when the first case was recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texasc_daily = texasc.diff().dropna().astype('int')\n",
    "texasc_daily.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the daily cases shows quite a large amount of variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texasc_daily.plot(kind='line', title=\"Texas - Daily Cases\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing methods\n",
    "\n",
    "A [wide variety of data smoothing methods exist][1], three of which we'll cover in detail.\n",
    "\n",
    "* Moving average\n",
    "* Weighted smoothing\n",
    "* Locally weighted scatterplot smoothing (LOWESS)\n",
    "\n",
    "[1]: https://en.wikipedia.org/wiki/Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving average\n",
    "\n",
    "A simple moving average takes a window of points around each point and calculates the average of those points as the new value for that date. The starting and ending window is chosen by the user. \n",
    "\n",
    "### Weekly seasonality\n",
    "\n",
    "The above graph of daily cases appears like what you would see monitoring a heart beat. There are spikes and dips at regular intervals. The cycles might appear on a weekly basis. **Seasonality** is a term that describes a regular repeating pattern in time series data. Let's get the average cases by day name to see if we can show that weekly seasonality occurs. All days should be roughly equal if there is no seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "texasc_daily.groupby(lambda x: x.day_name()).mean().loc[days].plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, a significant difference in the average between days exists. To account for this weekly seasonality, we will calculate a 7-day moving average. In, pandas, the moving average is calculated with the `rolling` method. Setting `center` to `True` chooses the three days preceding/following the current point and average them together. Here, we plot the smoothed and original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma = texasc_daily.rolling(7, min_periods=1, center=True).mean()\n",
    "texasc_daily.plot(kind='line', title=\"Texas - Daily Cases\", label='original')\n",
    "ma.plot(kind='line', label='7 day moving average').legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is substantially less variation, but still not a smooth line like we could draw by hand.\n",
    "\n",
    "### Repeated moving average\n",
    "\n",
    "We can take the moving average repeatedly to smooth the data further. Here, we complete three iterations of the 7-day moving average and produce an even smoother plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma = texasc_daily\n",
    "for _ in range(3):\n",
    "    ma = ma.rolling(7, min_periods=1, center=True).mean()\n",
    "\n",
    "texasc_daily.plot(title=\"Texas - Daily Cases\", label='original')\n",
    "ma.plot(label='7 day moving average (3 repeats)').legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a loop to plot differing number of repeats of the 7-day moving average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "texasc_daily.plot(title=\"Texas - Daily Cases\", label='original')\n",
    "repeats_to_plot = [0, 2, 4]\n",
    "ma = texasc_daily\n",
    "for i in range(5):\n",
    "    ma = ma.rolling(7, min_periods=1, center=True).mean()\n",
    "    if i in repeats_to_plot:\n",
    "        ma.plot(label=f'7-day moving average ({i + 1} repeats)').legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the smoothed cumulative total\n",
    "\n",
    "We can take the cumulative sum of the smoothed daily cases to get a smoothed cumulative total. Here, we repeat the smoothing process three times, take its cumulative sum, and plot it against the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma = texasc_daily\n",
    "for i in range(3):\n",
    "    ma = ma.rolling(7, min_periods=1, center=True).mean()\n",
    "\n",
    "texasc_smoothed = ma.cumsum()\n",
    "texasc.loc['2020-03-20':].plot(kind='line', label='original');\n",
    "texasc_smoothed.loc['2020-03-20':].plot(label='smoothed').legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted smoothing\n",
    "\n",
    "A slightly different technique involves taking a weighted average of the surround points with the points closer to the actual point given more weight. Here, we use a 7-day window and give 30% of the weight to the current observation, 15% each to the two nearest points, 12% each to the next two points, and 8% to the outer two points. This weighted moving average is plotted with the original moving average, so you can see the difference. In pandas, we had to create a custom function with `apply` to calculate the weighted sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = np.array([.08, .12, .15, .30, .15, .12, .08])\n",
    "def weighted_sum(x):\n",
    "    w = weight[:len(x)]\n",
    "    return (w * x).sum() / w.sum()\n",
    "    \n",
    "ma = texasc_daily.rolling(7, min_periods=1, center=True).mean()\n",
    "maw = texasc_daily.rolling(7, min_periods=1, center=True).apply(weighted_sum)\n",
    "ma.plot(label='7-day ma')\n",
    "maw.plot(label='7-day weighted ma').legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponentially weighted smoothing\n",
    "\n",
    "Instead of setting all the weights ourselves, we can use a function that exponentially decreases the weights with respect to their distance from the current point. \n",
    "\n",
    "$$w_i = \\alpha (1 - \\alpha) ^ i $$\n",
    "\n",
    "The above function calculates the weight at each point where $i$ represents the ith previous observation beginning at 0 from the current observation. The parameter $\\alpha$ determines the rate at which to discount previous observations. Higher values of $\\alpha$ place more importance on the closest observations.\n",
    "\n",
    "In pandas, we use the `ewm` method, passing it in the value of alpha. Let's output the first 10 observations and then calculate the exponentially weighted mean with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texasc_daily.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texasc_daily.ewm(alpha=.7).mean().head(10).round(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify by calculating manually\n",
    "\n",
    "Let's calculate the exponentially weighted average of the last observation by hand to help understand exponential smoothing better. First, we calculate the weights using the formula above. The weight array is reversed to align with the Series which has the last observation at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = .7\n",
    "w = a * (1 - a) ** np.arange(10)\n",
    "w = w[::-1]\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now take the weighted average to replicate the last result from pandas `ewm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(texasc_daily.head(10) * w).sum() / w.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texasc_daily.plot(title=\"Texas - Daily Cases\", label='original')\n",
    "texasc_daily.ewm(alpha=.1).mean().plot(kind='line', label=r'ewma $\\alpha=.1$')\n",
    "texasc_daily.ewm(alpha=.5).mean().plot(kind='line', label=r'ewma $\\alpha=.5$').legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue with the `ewm` method is that it does not allow you to center the calculation and use points on both sides of the current point. In order to do this, we'll reverse the Series, use `ewm` from the right side and then average the results together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = texasc_daily.ewm(alpha=.1).mean()\n",
    "right = texasc_daily[::-1].ewm(alpha=.1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's output the estimates for each to show the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to pandas automatic alignment of the index, we can add the two Series together knowing that the dates will align properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ewm_both = (left + right) / 2\n",
    "texasc_daily.plot(title=\"Texas - Daily Cases\", label='original')\n",
    "ewm_both.plot(kind='line', label=r'ewma combined $\\alpha=.1$').legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can repeat the procedure multiple times like we did above to generate much smoother data. This repetition allows us to use a higher alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texasc_daily.plot(title=\"Texas - Daily Cases\", label='original')\n",
    "n = [0, 2, 6]\n",
    "ewma = texasc_daily\n",
    "for i in range(30):\n",
    "    left = ewma.ewm(alpha=.5).mean()\n",
    "    right = ewma[::-1].ewm(alpha=.5).mean()\n",
    "    ewma = (left + right) / 2\n",
    "    if i in n:\n",
    "        ewma.plot(label=f'EWMA ({i + 1} repeats)').legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOWESS\n",
    "\n",
    "Locally weighted scatterplot smoothing, or LOWESS, is a procedure that also places more weight on the nearest observations. It fits a low-degree polynomial regression line through these weighted points. LOWESS is a computationally expensive procedure and not available directly in pandas. Below, we import the `lowess` function from the [statsmodels package][1].\n",
    "\n",
    "The main parameter is `frac` which is a number between 0 and 1 and determines the window size to consider as a fraction of the data. To be consistent as time goes on (and the length of our series increases), we'll choose a constant number of data points and use it to calculate `frac`. Here, we smooth based on 15 points. The last 10 smoothed points are shown below as a numpy array.\n",
    "\n",
    "[1]: https://www.statsmodels.org/stable/generated/statsmodels.nonparametric.smoothers_lowess.lowess.html#statsmodels.nonparametric.smoothers_lowess.lowess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "y = texasc_daily\n",
    "x = y.index\n",
    "frac = 20 / len(x)\n",
    "y_lowess = lowess(y, x, frac=frac, is_sorted=True, return_sorted=False)\n",
    "y_lowess[-10:].round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a Series out of this array and then plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_lowess = pd.Series(data=y_lowess, index=x)\n",
    "texasc_daily.plot(title=\"Texas - Daily Cases\", label='original')\n",
    "s_lowess.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothed cumulative total\n",
    "\n",
    "Let's compute the smoothed cumulative total from the smoothed daily values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texasc.loc['2020-03-20':].plot(kind='line', label='original');\n",
    "s_lowess_cumulative = s_lowess.cumsum().round(0).astype('int')\n",
    "ax = s_lowess_cumulative.plot(label='lowess', title='Original vs LOWESS')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aligning the cumulative total to the actual\n",
    "\n",
    "Smoothing the daily values has the effect of the cumulative total not being in direct alignment with the original data. Take a look at the last values from the actual and smoothed series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual\n",
    "last_actual = texasc.values[-1]\n",
    "last_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smoothed\n",
    "last_smoothed = s_lowess_cumulative.values[-1]\n",
    "last_smoothed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To align the two series, we'll multiply the smoothed values by the ratio of their last values. The new last smoothed cumulative value is output to verify it is equal to the last actual value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_lowess_cumulative = s_lowess_cumulative * last_actual / last_smoothed\n",
    "s_lowess_cumulative = s_lowess_cumulative.round(0).astype('int')\n",
    "s_lowess_cumulative.values[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing LOWESS for our model\n",
    "\n",
    "While the repeated average smoothing technique is simple, we'll use LOWESS as the smoother for our data. It is a more robust way to obtain the general direction of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 17\n",
    "\n",
    "<span style=\"color:green; font-size:16px\">Write a function to smooth the cumulative total given one Series of data using LOWESS. Allow the user to set `n`, the number of points used to produce the smoothed curve. Convert `n` to a fraction of the total points to use as the `frac` parameter in the `lowess` function. Return the smoothed cumulative total as Series with the same index as the original. Align the smoothed values so that the last value equals the last actual value.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(s, n):\n",
    "    \"\"\"\n",
    "    Smooth data using lowess function from statsmodels\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    s : Series of cumulative cases\n",
    "\n",
    "    n : int, number of points to be used by lowess function\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Series of smoothed values with same index as the original\n",
    "    \"\"\"\n",
    "    if s.values[0] == 0:\n",
    "        # Filter the data if the first value is 0\n",
    "        last_zero_date = s[s == 0].index[-1]\n",
    "        s = s.loc[last_zero_date:]\n",
    "        s_daily = s.diff().dropna()\n",
    "    else:\n",
    "        # If first value not 0, use it to fill in the \n",
    "        # first missing value\n",
    "        s_daily = s.diff().fillna(s.iloc[0])\n",
    "    \n",
    "    # Don't smooth data with less than 15 values\n",
    "    if len(s_daily) < 15:\n",
    "        return s\n",
    "    \n",
    "    pass # Begin writing your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verify that our function works by making the same plot as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solutions import smooth\n",
    "smoothed = smooth(texasc, 20)\n",
    "texasc.plot(label='Actual')\n",
    "smoothed.plot(title='Original vs Smoothed', label='Smoothed').legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we plot actual vs smoothed cases in California."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s = usa_cases['California']\n",
    "smoothed = smooth(s, 20)\n",
    "s.plot(label='Actual')\n",
    "smoothed.plot(title='California - Original vs Smoothed', label='Smoothed').legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
