{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encapsulation into Classes\n",
    "\n",
    "In this chapter, we'll encapsulate all of our work from the smoothing and model building chapters into Python classes. Encapsulation is just a fancy word for bringing together (capturing in a capsule, if you will) data and functions that act together to complete a task.\n",
    "\n",
    "## The `CasesModel` class\n",
    "\n",
    "We now write a class, `CasesModel`, that is responsible for performing nearly all tasks involving cases after downloading the data. For each area, it smooths, trains, and predicts the number of cases. It encapsulates all of the steps into a single class. Several of our previous functions (`smooth`, `get_bounds_p0`, `get_L_limits`, `predict`) have been rewritten as methods.\n",
    "\n",
    "Once instantiated, the `run` method must be called to complete the smoothing, training, and predicting. When the `run` method is called, a number of dictionaries are created to hold DataFrames with the following data:\n",
    "\n",
    "* smoothed data\n",
    "* bounds\n",
    "* initial parameter guess\n",
    "* fitted parameters\n",
    "* daily predicted cases\n",
    "* cumulative predicted cases\n",
    "* combined daily/cumulative actual and predicted cases\n",
    "* combined daily/cumulative smoothed actual and predicted cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import least_squares\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "plt.style.use('dashboard.mplstyle')\n",
    "\n",
    "GROUPS = 'world', 'usa'\n",
    "KINDS = 'cases', 'deaths'\n",
    "MIN_OBS = 15  # Minimum observations needed to make prediction\n",
    "\n",
    "def general_logistic_shift(x, L, x0, k, v, s):\n",
    "    return (L - s) / ((1 + np.exp(-k * (x - x0))) ** (1 / v)) + s\n",
    "\n",
    "def optimize_func(params, x, y, model):\n",
    "    y_pred = model(x, *params)\n",
    "    error = y - y_pred\n",
    "    return error\n",
    "\n",
    "class CasesModel:\n",
    "    def __init__(self, model, data, last_date, n_train, n_smooth, \n",
    "                 n_pred, L_n_min, L_n_max, **kwargs):\n",
    "        \"\"\"\n",
    "        Smooths, trains, and predicts cases for all areas\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        model : function such as general_logistic_shift\n",
    "        \n",
    "        data : dictionary of data from all areas - result of PrepareData().run()\n",
    "        \n",
    "        last_date : str, last date to be used for training\n",
    "        \n",
    "        n_train : int, number of preceding days to use for training\n",
    "        \n",
    "        n_smooth : integer, number of points used in LOWESS\n",
    "        \n",
    "        n_pred : int, days of predictions to make\n",
    "        \n",
    "        L_n_min, L_n_max : int, min/max number of days used to estimate L_min/L_max\n",
    "        \n",
    "        **kwargs : extra keyword arguments passed to scipy's least_squares function\n",
    "        \"\"\"\n",
    "        # Set basic attributes\n",
    "        self.model = model\n",
    "        self.data = data\n",
    "        self.last_date = self.get_last_date(last_date)\n",
    "        self.n_train = n_train\n",
    "        self.n_smooth = n_smooth\n",
    "        self.n_pred = n_pred\n",
    "        self.L_n_min = L_n_min\n",
    "        self.L_n_max = L_n_max\n",
    "        self.kwargs = kwargs\n",
    "        \n",
    "        # Set attributes for prediction\n",
    "        self.first_pred_date = pd.Timestamp(self.last_date) + pd.Timedelta(\"1D\")\n",
    "        self.pred_index = pd.date_range(start=self.first_pred_date, periods=n_pred)\n",
    "        \n",
    "    def get_last_date(self, last_date):\n",
    "        # Use the most current date as the last actual date if not provided\n",
    "        if last_date is None:\n",
    "            return self.data['world_cases'].index[-1]\n",
    "        else:\n",
    "            return pd.Timestamp(last_date)\n",
    "        \n",
    "    def init_dictionaries(self):\n",
    "        # Create dictionaries to store results for each area\n",
    "        # Executed first in `run` method\n",
    "        self.smoothed = {'world_cases': {}, 'usa_cases': {}}\n",
    "        self.bounds = {'world_cases': {}, 'usa_cases': {}}\n",
    "        self.p0 = {'world_cases': {}, 'usa_cases': {}}\n",
    "        self.params = {'world_cases': {}, 'usa_cases': {}}\n",
    "        self.pred_daily = {'world_cases': {}, 'usa_cases': {}}\n",
    "        self.pred_cumulative = {'world_cases': {}, 'usa_cases': {}}\n",
    "        \n",
    "        # Dictionary to hold DataFrame of actual and predicted values\n",
    "        self.combined_daily = {}\n",
    "        self.combined_cumulative = {}\n",
    "        \n",
    "        # Same as above, but stores smoothed and predicted values\n",
    "        self.combined_daily_s = {}\n",
    "        self.combined_cumulative_s = {}\n",
    "        \n",
    "    def smooth(self, s):\n",
    "        s = s[:self.last_date]\n",
    "        if s.values[0] == 0:\n",
    "            # Filter the data if the first value is 0\n",
    "            last_zero_date = s[s == 0].index[-1]\n",
    "            s = s.loc[last_zero_date:]\n",
    "            s_daily = s.diff().dropna()\n",
    "        else:\n",
    "            # If first value not 0, use it to fill in the \n",
    "            # first missing value\n",
    "            s_daily = s.diff().fillna(s.iloc[0])\n",
    "\n",
    "        # Don't smooth data with less than MIN_OBS values\n",
    "        if len(s_daily) < MIN_OBS:\n",
    "            return s_daily.cumsum()\n",
    "\n",
    "        y = s_daily.values\n",
    "        frac = self.n_smooth / len(y)\n",
    "        x = np.arange(len(y))\n",
    "        y_pred = lowess(y, x, frac=frac, is_sorted=True, return_sorted=False)\n",
    "        s_pred = pd.Series(y_pred, index=s_daily.index).clip(0)\n",
    "        s_pred_cumulative = s_pred.cumsum()\n",
    "        \n",
    "        if s_pred_cumulative[-1]  == 0:\n",
    "            # Don't use smoothed values if they are all 0\n",
    "            return s_daily.cumsum()\n",
    "        \n",
    "        last_actual = s.values[-1]\n",
    "        last_smoothed = s_pred_cumulative.values[-1]\n",
    "        s_pred_cumulative *= last_actual / last_smoothed\n",
    "        return s_pred_cumulative\n",
    "    \n",
    "    def get_train(self, smoothed):\n",
    "        # Filter the data for the most recent to capture new waves\n",
    "        return smoothed.iloc[-self.n_train:]\n",
    "    \n",
    "    def get_L_limits(self, s):\n",
    "        last_val = s[-1]\n",
    "        last_pct = s.pct_change()[-1] + 1\n",
    "        L_min = last_val * last_pct ** self.L_n_min\n",
    "        L_max = last_val * last_pct ** self.L_n_max + 1\n",
    "        L0 = (L_max - L_min) / 2 + L_min\n",
    "        return L_min, L_max, L0\n",
    "    \n",
    "    def get_bounds_p0(self, s):\n",
    "        L_min, L_max, L0 = self.get_L_limits(s)\n",
    "        x0_min, x0_max = -50, 50\n",
    "        k_min, k_max = 0.01, 0.5\n",
    "        v_min, v_max = 0.01, 2\n",
    "        s_min, s_max = 0, s[-1] + 0.01\n",
    "        s0 = s_max / 2\n",
    "        lower = L_min, x0_min, k_min, v_min, s_min\n",
    "        upper = L_max, x0_max, k_max, v_max, s_max\n",
    "        bounds = lower, upper\n",
    "        p0 = L0, 0, 0.1, 0.1, s0\n",
    "        return bounds, p0\n",
    "    \n",
    "    def train_model(self, s, bounds, p0):\n",
    "        y = s.values\n",
    "        n_train = len(y)\n",
    "        x = np.arange(n_train)\n",
    "        res = least_squares(optimize_func, p0, args=(x, y, self.model), bounds=bounds, **self.kwargs)\n",
    "        return res.x\n",
    "    \n",
    "    def get_pred_daily(self, n_train, params):\n",
    "        x_pred = np.arange(n_train - 1, n_train + self.n_pred)\n",
    "        y_pred = self.model(x_pred, *params)\n",
    "        y_pred_daily = np.diff(y_pred)\n",
    "        return pd.Series(y_pred_daily, index=self.pred_index)\n",
    "    \n",
    "    def get_pred_cumulative(self, s, pred_daily):\n",
    "        last_actual_value = s.loc[self.last_date]\n",
    "        return pred_daily.cumsum() + last_actual_value\n",
    "    \n",
    "    def convert_to_df(self, gk):\n",
    "        # convert dictionary of areas mapped to Series to DataFrames\n",
    "        self.smoothed[gk] = pd.DataFrame(self.smoothed[gk]).fillna(0).astype('int')\n",
    "        self.bounds[gk] = pd.concat(self.bounds[gk].values(), \n",
    "                                    keys=self.bounds[gk].keys()).T\n",
    "        self.bounds[gk].loc['L'] = self.bounds[gk].loc['L'].round()\n",
    "        self.p0[gk] = pd.DataFrame(self.p0[gk], index=['L', 'x0', 'k', 'v', 's'])\n",
    "        self.p0[gk].loc['L'] = self.p0[gk].loc['L'].round()\n",
    "        self.params[gk] = pd.DataFrame(self.params[gk], index=['L', 'x0', 'k', 'v', 's'])\n",
    "        self.pred_daily[gk] = pd.DataFrame(self.pred_daily[gk])\n",
    "        self.pred_cumulative[gk] = pd.DataFrame(self.pred_cumulative[gk])\n",
    "        \n",
    "    def combine_actual_with_pred(self):\n",
    "        for gk, df_pred in self.pred_cumulative.items():\n",
    "            df_actual = self.data[gk][:self.last_date]\n",
    "            df_comb = pd.concat((df_actual, df_pred))\n",
    "            self.combined_cumulative[gk] = df_comb\n",
    "            self.combined_daily[gk] = df_comb.diff().fillna(df_comb.iloc[0]).astype('int')\n",
    "            \n",
    "            df_comb_smooth = pd.concat((self.smoothed[gk], df_pred))\n",
    "            self.combined_cumulative_s[gk] = df_comb_smooth\n",
    "            self.combined_daily_s[gk] = df_comb_smooth.diff().fillna(df_comb.iloc[0]).astype('int')\n",
    "\n",
    "    def run(self):\n",
    "        self.init_dictionaries()\n",
    "        for group in GROUPS:\n",
    "            gk = f'{group}_cases'\n",
    "            df_cases = self.data[gk]\n",
    "            for area, s in df_cases.items():\n",
    "                smoothed = self.smooth(s)\n",
    "                train = self.get_train(smoothed)\n",
    "                n_train = len(train)\n",
    "                if n_train < MIN_OBS:\n",
    "                    bounds = np.full((2, 5), np.nan)\n",
    "                    p0 = np.full(5, np.nan)\n",
    "                    params = np.full(5, np.nan)\n",
    "                    pred_daily = pd.Series(np.zeros(self.n_pred), index=self.pred_index)\n",
    "                else:\n",
    "                    bounds, p0 = self.get_bounds_p0(train)\n",
    "                    params = self.train_model(train, bounds=bounds,  p0=p0)\n",
    "                    pred_daily = self.get_pred_daily(n_train, params).round(0)\n",
    "                pred_cumulative = self.get_pred_cumulative(s, pred_daily)\n",
    "                \n",
    "                # save results to dictionaries mapping each area to its result\n",
    "                self.smoothed[gk][area] = smoothed\n",
    "                self.bounds[gk][area] = pd.DataFrame(bounds, index=['lower', 'upper'], \n",
    "                                                     columns=['L', 'x0', 'k', 'v', 's'])\n",
    "                self.p0[gk][area] = p0\n",
    "                self.params[gk][area] = params\n",
    "                self.pred_daily[gk][area] = pred_daily.astype('int')\n",
    "                self.pred_cumulative[gk][area] = pred_cumulative.astype('int')\n",
    "            self.convert_to_df(gk)\n",
    "        self.combine_actual_with_pred()\n",
    "        \n",
    "    def plot_prediction(self, group, area, **kwargs):\n",
    "        group_kind = f'{group}_cases'\n",
    "        actual = self.data[group_kind][area]\n",
    "        pred = self.pred_cumulative[group_kind][area]\n",
    "        first_date = self.last_date - pd.Timedelta(self.n_train, 'D')\n",
    "        last_pred_date = self.last_date + pd.Timedelta(self.n_pred, 'D')\n",
    "        actual.loc[first_date:last_pred_date].plot(label='Actual', **kwargs)\n",
    "        pred.plot(label='Predicted').legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An instance of the `CasesModel` class is created below. It uses the 60 days leading up to November 5, 2020 as training data for the model and makes predictions for the next 30 days. If `last_date` is not provided, then the last date from the given data is used. The integers `L_n_min` and `L_n_max` are used to find the bounds of `L`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prepare import PrepareData\n",
    "data = PrepareData(download_new=False).run()\n",
    "cm = CasesModel(model=general_logistic_shift, data=data, last_date='2020-11-05', \n",
    "                n_train=60, n_smooth=15, n_pred=30, L_n_min=5, L_n_max=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `run` method must be called in order to smooth, train, and predict. Executing the following cell took about 25 seconds on my machine, as it completed the process for all areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cm.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Let's take a look at all of the results which are stored as DataFrames within dictionaries with keys `world_cases` and `usa_cases`. The original unprocessed data is in the `data` attribute. We select the last five rows of the first 10 areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.data['world_cases'].iloc[-5:, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The smoothed data is what is used for training and is therefore only calculated through the date we wish to make a prediction from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.smoothed['usa_cases'].iloc[-5:, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bounds for each of the parameters when fitting are below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.bounds['world_cases'].iloc[:, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial guess for each parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.p0['world_cases'].iloc[:, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first five predicted values for daily cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.pred_daily['usa_cases'].iloc[:5, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first five predicted values for cumulative cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.pred_cumulative['usa_cases'].iloc[:5, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `combined_daily` attribute contains the actual and predicted values combined in a single DataFrame. Below, we have the last three days of actual data and the first three predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.combined_daily['usa_cases'].loc['2020-11-03':'2020-11-08', :'Delaware']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the `combined_cumulative` dictionary holds the actual cumulative along with the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.combined_cumulative['usa_cases'].loc['2020-11-03':'2020-11-08', :'Delaware']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `combined_daily_s` and `combined_cumulative_s` have the smoothed actual values with the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cm.combined_daily_s['usa_cases'].loc['2020-11-03':'2020-11-08', :'Delaware']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.combined_cumulative_s['usa_cases'].loc['2020-11-03':'2020-11-08', :'Delaware']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting results\n",
    "\n",
    "A `plot_prediction` method was also defined to visualize the actual vs predicted values for a given area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.plot_prediction('usa', 'Texas', title=\"Texas Cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Prediction for Deaths using Case Fatality Ratio\n",
    "\n",
    "We've only discussed making predictions for cases and not deaths. We could use a generalized logistic function to model deaths and get reasonable results, though a  simpler approach exists using the Case Fatality Ratio or CFR. The CFR is the fraction of cases resulting in death. Knowing this ratio can help us estimate the number of deaths that result from the number of recorded cases. While the CFR can change substantially over the course of a pandemic as treatments change and more people have access to tests, it should be reasonably stable over a short amount of time.\n",
    "\n",
    "From clinical data, deaths usually occur two to three weeks after the initial coronavirus infection. Using this knowledge, we can estimate the CFR based on historical cases and deaths. To calculate the CFR, we do the following:\n",
    "\n",
    "* Find total cases between 15 and 45 days prior\n",
    "* Find total deaths between 0 and 30 days prior\n",
    "* Divide the total cases by the total deaths\n",
    "\n",
    "The function below takes the unprocessed data and the last date of known values and then calculates the CFR for each area. A CFR of 0.005 is used for countries that have no cases in the last 30 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cfr(data, last_date):\n",
    "    last_day_deaths = last_date\n",
    "    first_day_deaths = last_date - pd.Timedelta('30D')\n",
    "    last_day_cases = last_day_deaths - pd.Timedelta('15D')\n",
    "    first_day_cases = last_day_cases - pd.Timedelta('30D')\n",
    "    \n",
    "    cfr = {}\n",
    "    for group in GROUPS:\n",
    "        deaths, cases = data[f'{group}_deaths'], data[f'{group}_cases']\n",
    "        deaths_total = deaths.loc[last_day_deaths] - deaths.loc[first_day_deaths]\n",
    "        cases_total = cases.loc[last_day_cases] - cases.loc[first_day_cases]\n",
    "        cfr[group] = (deaths_total / cases_total).fillna(0.005)\n",
    "    return cfr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the function to get the CFR for all areas and output some of the calculated values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_date = pd.Timestamp('2020-11-05')\n",
    "cfr = calculate_cfr(data, last_date)\n",
    "cfr['world'].head(10).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfr['usa'].head(10).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create class to model deaths\n",
    "\n",
    "We create another class, `DeathsModel`, to model the deaths of each area. It allows the user to set the `lag`, number of days between cases and deaths, and the `period`, number of days to tabulate the total cases/deaths for the CFR calculation. The `predict` method multiplies the CFR by the number of cases that happened `lag` days ago. For example, if we want to predict the number of deaths on November 6, we look back at the number of cases on October 22 (assuming the lag is 15) and multiply this number by the CFR of that area. To help get smoother results, we use a 7-day rolling average instead of the actual value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeathsModel:\n",
    "    def __init__(self, data, last_date, cm, lag, period):\n",
    "        \"\"\"\n",
    "        Build simple model based on CFR to predict deaths for all areas\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : dictionary of data from all areas - result of PrepareData().run()\n",
    "\n",
    "        last_date : str, last date to be used for training\n",
    "\n",
    "        cm : CasesModel instance after calling `run` method\n",
    "        \n",
    "        lag : int, number of days between cases and deaths, used to calculate CFR\n",
    "        \n",
    "        period : int, window size of number of days to calculate CFR\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.last_date = self.get_last_date(last_date)\n",
    "        self.cm = cm\n",
    "        self.lag = lag\n",
    "        self.period = period\n",
    "        self.pred_daily = {}\n",
    "        self.pred_cumulative = {}\n",
    "        \n",
    "        # Dictionary to hold DataFrame of actual and predicted values\n",
    "        self.combined_daily = {}\n",
    "        self.combined_cumulative = {}\n",
    "        \n",
    "    def get_last_date(self, last_date):\n",
    "        if last_date is None:\n",
    "            return self.data['world_cases'].index[-1]\n",
    "        else:\n",
    "            return pd.Timestamp(last_date)\n",
    "        \n",
    "    def calculate_cfr(self):\n",
    "        first_day_deaths = self.last_date - pd.Timedelta(f'{self.period}D')\n",
    "        last_day_cases = self.last_date - pd.Timedelta(f'{self.lag}D')\n",
    "        first_day_cases = last_day_cases - pd.Timedelta(f'{self.period}D')\n",
    "\n",
    "        cfr = {}\n",
    "        for group in GROUPS:\n",
    "            deaths = self.data[f'{group}_deaths']\n",
    "            cases = self.data[f'{group}_cases']\n",
    "            deaths_total = deaths.loc[self.last_date] - deaths.loc[first_day_deaths]\n",
    "            cases_total = cases.loc[last_day_cases] - cases.loc[first_day_cases]\n",
    "            cfr[group] = (deaths_total / cases_total).fillna(0.01)\n",
    "        return cfr\n",
    "    \n",
    "    def run(self):\n",
    "        self.cfr = self.calculate_cfr()\n",
    "        for group in GROUPS:\n",
    "            group_cases = f'{group}_cases'\n",
    "            group_deaths = f'{group}_deaths'\n",
    "            cfr_start_date = self.last_date - pd.Timedelta(f'{self.lag}D')\n",
    "            \n",
    "            daily_cases_smoothed = self.cm.combined_daily_s[group_cases]\n",
    "            pred_daily = daily_cases_smoothed[cfr_start_date:] * self.cfr[group]\n",
    "            pred_daily = pred_daily.iloc[:self.cm.n_pred]\n",
    "            pred_daily.index = self.cm.pred_daily[group_cases].index\n",
    "            \n",
    "            # Use repeated rolling average to smooth out the predicted deaths\n",
    "            for i in range(5):\n",
    "                pred_daily = pred_daily.rolling(14, min_periods=1, center=True).mean()\n",
    "            \n",
    "            pred_daily = pred_daily.round(0).astype(\"int\")\n",
    "            self.pred_daily[group_deaths] = pred_daily\n",
    "            last_deaths = self.data[group_deaths].loc[self.last_date]\n",
    "            self.pred_cumulative[group_deaths] = pred_daily.cumsum() + last_deaths\n",
    "        self.combine_actual_with_pred()\n",
    "            \n",
    "    def combine_actual_with_pred(self):\n",
    "        for gk, df_pred in self.pred_cumulative.items():\n",
    "            df_actual = self.data[gk][:self.last_date]\n",
    "            df_comb = pd.concat((df_actual, df_pred))\n",
    "            self.combined_cumulative[gk] = df_comb\n",
    "            self.combined_daily[gk] = df_comb.diff().fillna(df_comb.iloc[0]).astype('int')\n",
    "            \n",
    "    def plot_prediction(self, group, area, **kwargs):\n",
    "        group_kind = f'{group}_deaths'\n",
    "        actual = self.data[group_kind][area]\n",
    "        pred = self.pred_cumulative[group_kind][area]\n",
    "        first_date = self.last_date - pd.Timedelta(60, 'D')\n",
    "        last_pred_date = self.last_date + pd.Timedelta(30, 'D')\n",
    "        actual.loc[first_date:last_pred_date].plot(label='Actual', **kwargs)\n",
    "        pred.plot(label='Predicted').legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate this class and then call the `run` method, which should execute immediately as the model for deaths is far simpler than it is for cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DeathsModel(data=data, last_date='2020-11-05', cm=cm, lag=15, period=30)\n",
    "dm.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's output the first daily and cumulative predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.pred_daily['usa_deaths'].iloc[:5, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.pred_cumulative['usa_deaths'].iloc[:5, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as with the cases, `combined_daily` and `combined_cumulative` are available combining actual and predicted values. Again, we look at the three days preceding and following the predicted date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.combined_daily['usa_deaths'].loc['2020-11-03':'2020-11-08', :'Delaware']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.combined_cumulative['usa_deaths'].loc['2020-11-03':'2020-11-08', :'Delaware']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `plot_prediction` method to plot the actual and predicted values of deaths for a particular area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dm.plot_prediction('usa', 'Texas', title='Texas Deaths')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating final tables for the dashboard\n",
    "\n",
    "Now that we have all the information for our dashboard, we'll need to extract it from these classes and store them as files. For simplicity, we will store them as CSV files.\n",
    "\n",
    "### Adding USA to world tables\n",
    "\n",
    "Originally, we filtered out the USA from the world tables. We did this knowing that the predictions for the entire country were likely to be different than the sum of the predictions of each individual state. Now that we have each state's predictions, we will total them up and append them to the world tables. Here, the daily and cumulative cases and deaths are assigned to new variables. Each of the world DataFrames has the USA added as a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Daily Cases and Deaths from dictionaries\n",
    "world_cases_d = cm.combined_daily['world_cases']\n",
    "usa_cases_d = cm.combined_daily['usa_cases']\n",
    "world_deaths_d = dm.combined_daily['world_deaths']\n",
    "usa_deaths_d = dm.combined_daily['usa_deaths']\n",
    "\n",
    "# Add USA to world \n",
    "world_cases_d = world_cases_d.assign(USA=usa_cases_d.sum(axis=1))\n",
    "world_deaths_d = world_deaths_d.assign(USA=usa_deaths_d.sum(axis=1))\n",
    "\n",
    "# Get Cumulative Cases and Deaths\n",
    "world_cases_c = cm.combined_cumulative['world_cases']\n",
    "usa_cases_c = cm.combined_cumulative['usa_cases']\n",
    "world_deaths_c = dm.combined_cumulative['world_deaths']\n",
    "usa_deaths_c = dm.combined_cumulative['usa_deaths']\n",
    "\n",
    "# Add USA to world \n",
    "world_cases_c = world_cases_c.assign(USA=usa_cases_c.sum(axis=1))\n",
    "world_deaths_c = world_deaths_c.assign(USA=usa_deaths_c.sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verify that the USA has been added to the world cases DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_cases_d.iloc[-5:, -10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a single table to store results\n",
    "\n",
    "A single table will be used to hold the daily and cumulative cases and deaths for each area for each date. We'll reshape the DataFrames using the `stack` method so that all values are in a single column with the index containing the date and the area name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_cases_d.stack().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can place all four Series as columns in a single DataFrame using the `concat` function using the `keys` parameter to label each new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_world = pd.concat((world_deaths_d.stack(), world_cases_d.stack(), \n",
    "                    world_deaths_c.stack(), world_cases_c.stack()), axis=1, \n",
    "                   keys=['Daily Deaths', 'Daily Cases', 'Deaths', 'Cases'])\n",
    "df_world.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same thing is done for the USA data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usa = pd.concat((usa_deaths_d.stack(), usa_cases_d.stack(), \n",
    "                    usa_deaths_c.stack(), usa_cases_c.stack()), axis=1, \n",
    "                   keys=['Daily Deaths', 'Daily Cases', 'Deaths', 'Cases'])\n",
    "df_usa.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the above code is placed in a function that accepts instances of the `CasesModel` and `DeathsModel` as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_all_data(cm, dm):\n",
    "    # Get Daily Cases and Deaths\n",
    "    world_cases_d = cm.combined_daily['world_cases']\n",
    "    usa_cases_d = cm.combined_daily['usa_cases']\n",
    "    world_deaths_d = dm.combined_daily['world_deaths']\n",
    "    usa_deaths_d = dm.combined_daily['usa_deaths']\n",
    "\n",
    "    # Add USA to world \n",
    "    world_cases_d = world_cases_d.assign(USA=usa_cases_d.sum(axis=1))\n",
    "    world_deaths_d = world_deaths_d.assign(USA=usa_deaths_d.sum(axis=1))\n",
    "\n",
    "    # Get Cumulative Cases and Deaths\n",
    "    world_cases_c = cm.combined_cumulative['world_cases']\n",
    "    usa_cases_c = cm.combined_cumulative['usa_cases']\n",
    "    world_deaths_c = dm.combined_cumulative['world_deaths']\n",
    "    usa_deaths_c = dm.combined_cumulative['usa_deaths']\n",
    "\n",
    "    # Add USA to world \n",
    "    world_cases_c = world_cases_c.assign(USA=usa_cases_c.sum(axis=1))\n",
    "    world_deaths_c = world_deaths_c.assign(USA=usa_deaths_c.sum(axis=1))\n",
    "    \n",
    "    df_world = pd.concat((world_deaths_d.stack(), world_cases_d.stack(), \n",
    "                          world_deaths_c.stack(), world_cases_c.stack()), axis=1, \n",
    "                         keys=['Daily Deaths', 'Daily Cases', 'Deaths', 'Cases'])\n",
    "    \n",
    "    df_usa = pd.concat((usa_deaths_d.stack(), usa_cases_d.stack(), \n",
    "                        usa_deaths_c.stack(), usa_cases_c.stack()), axis=1, \n",
    "                       keys=['Daily Deaths', 'Daily Cases', 'Deaths', 'Cases'])\n",
    "    df_all = pd.concat((df_world, df_usa), keys=['world', 'usa'], \n",
    "                       names=['group', 'date', 'area'])\n",
    "    df_all.to_csv('data/all_data.csv')\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the very end of this function concatenates the world and USA DataFrames one on top of each other and adds a new index level 'group' to the DataFrame. The data is written to the file `all_data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = combine_all_data(cm, dm)\n",
    "df_all.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create summary table\n",
    "\n",
    "The main table in our dashboard is a summary of the data at the current date. We'll create it now and begin by selecting the current day's rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary = df_all.query('date == @last_date')\n",
    "df_summary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read in a file called population.csv that has the population and code (used in the map) of each area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop = pd.read_csv(\"data/population.csv\")\n",
    "pop.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's merge these two tables together and add columns for deaths and cases per million."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary = df_summary.merge(pop, how='left', on=['group','area'])\n",
    "df_summary[\"Deaths per Million\"] = (df_summary[\"Deaths\"] / df_summary[\"population\"]).round(0)\n",
    "df_summary[\"Cases per Million\"] = (df_summary[\"Cases\"] / df_summary[\"population\"]).round(-1)\n",
    "df_summary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's place all of this code within its own function which also writes the data to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_table(df_all, last_date):\n",
    "    df = df_all.query('date == @last_date')\n",
    "    pop = pd.read_csv(\"data/population.csv\")\n",
    "    df = df.merge(pop, how='left', on=['group','area'])\n",
    "    df[\"Deaths per Million\"] = (df[\"Deaths\"] / df[\"population\"]).round(0)\n",
    "    df[\"Cases per Million\"] = (df[\"Cases\"] / df[\"population\"]).round(-1)\n",
    "    df['date'] = last_date\n",
    "    df.to_csv('data/summary.csv', index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_summary_table(df_all, last_date).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code within the modules\n",
    "\n",
    "The `CasesModel` and `DeathsModel` class are placed in the `models.py` file. The `PrepareData` class and `combine_all_data` and `create_summary_table` functions are placed in the `prepare.py` file. In the next chapter, we'll run all of our code for the entire project to prepare the data, make predictions, and save the final tables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
